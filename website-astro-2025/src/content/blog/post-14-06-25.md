---
title: "Shock! LLM's are bad at some things and Apple is the only one willing to tell us"
pubDate: 2025-06-14
description: "The companies selling us these things want us to think of them as conscious. But thinking of language models this way is redutictive and doesn't give them credit for what they're actually good for."
author: "David Ross"
image:
  url: "https://i.guim.co.uk/img/media/44856f0611a6ae5e8f4e1875772d8f462a7e6f10/0_139_1118_671/master/1118.jpg?width=1200&quality=85&auto=format&fit=max&s=879e85646e59ea6a7aab83a1e66ab506"
  alt: ""
tags: ["LLMs", "technology", "neuroscience"]
slug: post-14-06-25
---

<img 
  src="https://images.pexels.com/photos/17483868/pexels-photo-17483868/free-photo-of-an-artist-s-illustration-of-artificial-intelligence-ai-this-image-represents-how-machine-learning-is-inspired-by-neuroscience-and-the-human-brain-it-was-created-by-novoto-studio-as-par.jpeg?auto=compress&cs=tinysrgb&w=1260&h=750&dpr=2"
  alt="An artistâ€™s illustration of artificial intelligence, inspired by neuroscience and the human brain."
  style="max-width:80%; height:auto; display:block; margin: 0em auto 1em auto; border-radius: 1em;"
/>

As of writing in June 2025 LLM's (Large Language Models) have already transformed the way people access information on the web. "AI" as it is broadly know is in fact just an immensely complex predictive text algorithm trained on unfathomable amounts of data scraped from the web. The result of these models being packaged as a chatbot interface can look a lot like intelligence. The other indisputable truth of 2025 is that LLMs are being crowbarred into nearly every aspect of out technological lives and that their implementation frequently falls short of the sales pitch. Products like the Humane AI Pin are almost too easy a target but here's a [video](https://www.youtube.com/watch?v=9lNIwOOMVHk) just as a reminder of what they said their product could do. Needless to say the reality didn't live up to this and Human Inc. is now defunct.

Microsoft, Google, Facebook and Amazon are all betting big on AI. The fear of missing out on the next big tech innovation is driving a lot of the rabid AI enthusiasm. But the ability to charge for it as a premium subscription service must also be hard to resist. In this environment Apple seems like an outsider. The only giant tech conglomerate not jumping head first into the AI future.
Despite their enthusiastic [uptake in 2024](https://www.youtube.com/watch?v=RXeOiIDNNek) many of the promised features - improved Siri, etc. - have yet to eventuate with Apple stating that it needed more time to [meet their quality standards](https://www.youtube.com/watch?v=NTLk53h7u_k). The 2025 Apple Developer Conference (WWDC) was comparatively light on "AI" - the word "Siri" was mentioned in passing only twice. So what's going on at Apple?

To understanding their hand-wringing I think it's important to look at what their incentives are compared to the other tech giants. They are unique in that their business model is centred on the sale of premium hardware devices. Every other company in this space is software-first. Apple, despite it feeling like the default to many people only represents about 30% of the global phone market. So what? so they have to be careful. Their value is based on the perception of quality. If they put a half baked LLM in their tightly controlled ecosystem that perception will suffer.

In the same week as WWDC 2025 Apple published an academic [paper](https://ml-site.cdn-apple.com/papers/the-illusion-of-thinking.pdf) on their [website](https://machinelearning.apple.com/). In the paper the authors suggest problems with the way Large Reasoning Models (LRMs) are measured. Suggesting that the emphasis on mathematical and coding benchmarks fail to assess the quality of a models "reasoning". Instead, the authors pit models against a series of puzzles such as the [Tower of Hanoi](https://en.wikipedia.org/wiki/Tower_of_Hanoi) and the [River Crossing Puzzle](https://en.wikipedia.org/wiki/River_crossing_puzzle). They found that beyond a certain threshold of complexity their ability to reason their way through and solve problems tends to break down.

> "This behavior suggests a fundamental scaling
> limitation in the thinking capabilities of current reasoning models relative to problem complexity."

This isn't a new observation of LRMs or LLMs. They're good at many things but once they lose the plot they do so spectacularly badly and with the confidence of a huckster trying to steal your wallet. But the Apple paper suggests this problem isn't going away even with the latest and greatest models optimised for "reasoning" behaviours.

The gamble at the moment seems to be that as we throw more compute power and more data at these language models they'll be able to solve more and more complex problems to the point where we can rely on them for just about everything. But this isn't clear.

<iframe
src="https://www.youtube.com/embed/dDUC-LqVrPU"
title="Has Generative AI Already Peaked? - Computerphile"
style="width: 90%; aspect-ratio: 16/9; max-width: 900px; display: block; margin: 2em auto;"
frameborder="0"
allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture; web-share"
allowfullscreen> </iframe>

This video from the YouTube channel Computerphile outlines this problem better than I can. But the upshot is that the unbridled progress of LLMs is not by any means a given.

---
